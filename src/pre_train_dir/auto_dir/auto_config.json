{
  "factor": 1,
  "dropout": 0.05,
  "output_attention": true,
  "d_ff": 2048,
  "moving_avg": 25,
  "activation": "gelu",
  "num_attention_heads": 6,
  "pre_seq_len": 336,
  "num_hidden_layers": 6,
  "hidden_size": 768,
  "latent_dim": 32,
  "prefix_projection": true,
  "prefix_hidden_size": 336,
  "n_batch": 6,
  "pre-train": false,
  "type_vocab_size": 2,
  "vocab_size": 4101,
  "max_position_embeddings": 512,
  "layer_norm_eps": 1e-12,
  "hidden_dropout_prob": 0.1,
  "hidden_act": "gelu"
}